{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['balanced', 'byclass', 'bymerge', 'digits', 'letters', 'mnist']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emnist\n",
    "from emnist import list_datasets\n",
    "list_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emnist import extract_training_samples\n",
    "global images, labels\n",
    "images, labels = extract_training_samples('byclass')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(images[78])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(697932, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "#images = np.reshape()\n",
    "print(images.shape)\n",
    "plt.imshow(images[543])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = 0\n",
    "en = 0\n",
    "bs = 697932\n",
    "global model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape = (1,28,28)))\n",
    "model.add(tf.keras.layers.Dense(256 ,activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(256, activation = tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(62, activation = tf.nn.softmax))\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy')#, matrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 62)                15934     \n",
      "=================================================================\n",
      "Total params: 282,686\n",
      "Trainable params: 282,686\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(beg, end):\n",
    "    loc_img = images[beg:end]\n",
    "    loc_lab = labels[beg:end]\n",
    "    #loc_img = loc_img.reshape(-1,784)\n",
    "    loc_img = tf.keras.utils.normalize(loc_img, axis = 1)\n",
    "    loc_lab = tf.keras.utils.to_categorical(loc_lab,62)\n",
    "    #for i in 1000:\n",
    "    model.fit(loc_img,loc_lab, epochs = 1)\n",
    "\n",
    "    \n",
    "    del loc_img, loc_lab\n",
    "#loc = images[0]\n",
    "#plt.imshow(loc)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 28, 28) for input Tensor(\"flatten_input:0\", shape=(None, 1, 28, 28), dtype=float32), but it was called on an input with incompatible shape (None, 28, 28).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 28, 28) for input Tensor(\"flatten_input:0\", shape=(None, 1, 28, 28), dtype=float32), but it was called on an input with incompatible shape (None, 28, 28).\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 3.5641\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 2.5041\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 2.0807\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 1.7050\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 1.5771\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 1.5443\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 1.4540\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 1.3616\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 1.3948\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 1.2493\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 1.2572\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 1.2360\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 1.2779\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 1.2403\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 1.2227\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 1.1811\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 1.0878\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 1.1554\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 1.0600\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 1.0151\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 1.0709\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 1.0581\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.9751\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 1.0164\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 1.0372\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.9982\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.9123\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 1.0277\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.9199\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 1.0027\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.9169\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.9390\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8553\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.9447\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 1.0123\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8670\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8678\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.9788\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.8834\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8726\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8392\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.8278\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7997\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8508\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8479\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8765\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8166\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8540\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8173\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7684\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8815\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8505\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8503\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7781\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8247\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.8133\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8024\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8142\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8075\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8132\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8413\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7804\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8461\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7942\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7577\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7886\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7314\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7400\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8027\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7652\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7337\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7472\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7609\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7204\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7377\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7560\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7710\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.7879\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.7655\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7119\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.7289\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.7071\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.7126\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6775\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6625\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7453\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7406\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7385\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.7740\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7173\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7518\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6847\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7204\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6992\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.7340\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7238\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7029\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6393\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6359\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7071\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7093\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.7349\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6510\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.7830\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6859\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6721\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6664\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6972\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7127\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7093\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6534\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7011\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.7159\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.7036\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5744\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.6918\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6902\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.6411\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6495\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.6930\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.6474\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.6545\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.6631\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6412\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6291\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6082\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7039\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6013\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5760\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7403\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7224\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7280\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6185\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6485\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6666\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6232\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6464\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5918\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6190\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6561\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6845\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6733\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6604\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6079\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6553\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6431\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6328\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6261\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6091\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6586\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6356\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6056\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6351\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6131\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6484\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6234\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6201\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6139\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6618\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5493\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6419\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6006\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6344\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6048\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5717\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5985\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6010\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5828\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6448\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5461\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6383\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6853\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5834\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6267\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5705\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5996\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6402\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6495\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5985\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7019\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6901\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6609\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5880\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6597\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6403\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5572\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6064\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6599\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6036\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5790\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6211\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6325\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6132\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6058\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5687\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5680\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5909\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5395\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5965\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5977\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5997\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6173\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6454\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5884\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5934\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5779\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6061\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.5924\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6095\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6004\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6679\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5544\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6149\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5722\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5871\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.4943\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5458\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5983\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5825\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6476\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5470\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5922\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6324\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5954\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5721\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5389\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5980\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5978\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 10ms/step - loss: 0.6366\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5577\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.6040\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5878\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.6466\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.6284\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5828\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5761\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5359\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5430\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5951\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5254\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5727\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.6018\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5946\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5462\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5472\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5971\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5878\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5579\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5320\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5981\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5727\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.6188\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5956\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.6284\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5171\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5837\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5556\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5610\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5504\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6261\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.4705\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.5237\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5452\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5576\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5750\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5677\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5632\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5647\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6103\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.5432\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5458\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6048\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5734\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6086\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5802\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5400\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5288\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5638\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6036\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5838\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5301\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5788\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5854\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5718\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5609\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5441\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6035\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6103\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5507\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5835\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5670\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5006\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5320\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6040\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5846\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5775\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5485\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4943\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5505\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5129\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5910\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5721\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5798\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5485\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5790\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5319\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6249\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5085\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5356\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5508\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5393\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5371\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5586\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5169\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5336\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5376\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5520\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5442\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5432\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6183\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5408\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5269\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5616\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4881\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.6029\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.6088\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6048\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5510\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5796\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5079\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5684\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5253\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5461\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5739\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6151\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5057\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5255\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4778\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4760\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5260\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5081\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6305\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5498\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5958\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5551\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5362\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5447\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4862\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.5858\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5761\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.5495\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5387\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5504\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5656\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5467\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5598\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.6010\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.5629\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5369\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.5886\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5082\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5004\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.4820\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5988\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5297\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.5799\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5820\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5909\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5424\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5826\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5068\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5858\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5723\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5281\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.5789\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.4789\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5545\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.6019\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5186\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5292\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.5276\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5058\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5625\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5159\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5247\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.4969\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5678\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.5738\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5810\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5726\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5658\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5538\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5196\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5251\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5860\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5197\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5441\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.5361\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.5229\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.5744\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5505\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5372\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4906\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.6274\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5555\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5727\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5573\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5325\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5344\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5307\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5120\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4463\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.5527\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.4957\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5366\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5414\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5582\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5372\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.4694\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5665\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5666\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.4913\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5494\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5090\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.4784\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5806\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4988\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5489\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5333\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.5156\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5402\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5052\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5022\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.5698\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5353\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5322\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5133\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5179\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5325\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5073\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4734\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5262\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5250\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5455\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.4812\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5227\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5745\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5478\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5145\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5102\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5554\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5522\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5710\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5095\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5616\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5144\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5553\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5321\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5663\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5825\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5130\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5450\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5117\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4692\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5593\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5548\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5494\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5627\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5196\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4631\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5393\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5210\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5595\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5646\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5150\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5458\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5292\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5423\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5356\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5316\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4813\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5850\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5637\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5145\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4994\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4752\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5462\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5548\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4916\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5076\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5220\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5449\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5776\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5255\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4969\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5428\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4624\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.6009\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4755\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4925\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5878\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5167\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5346\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5584\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5707\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5307\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4851\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4787\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4982\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5817\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5572\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5317\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4833\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4988\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4954\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5481\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5199\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5255\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4905\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4920\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5079\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4963\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5523\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4841\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5129\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4825\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5336\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5308\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5355\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5424\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5109\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5168\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5215\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4891\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5359\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4826\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5008\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4889\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5137\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5251\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5101\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4957\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5843\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4904\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5005\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4781\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5348\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4850\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5237\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4828\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4868\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4980\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5756\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5043\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5429\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5288\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.6042\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4740\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5749\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5006\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5423\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5635\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5311\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5063\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5104\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5076\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5255\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5189\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.4849\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5559\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5219\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5465\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5098\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5132\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5747\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5617\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4815\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5153\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5135\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5165\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4980\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4812\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4905\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5364\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4679\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5224\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4303\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4984\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5384\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5436\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5184\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5184\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5329\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4541\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5405\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4803\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4899\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5020\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5428\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5105\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5188\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4629\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5483\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5319\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.6001\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4971\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4958\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4870\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5976\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5176\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4851\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5061\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5202\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4949\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5044\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5030\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4878\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5539\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4871\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5117\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4941\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4913\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4852\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4980\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4931\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5485\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5320\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5911\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5461\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5501\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4918\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4960\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5113\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4846\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4911\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4892\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5538\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4840\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5022\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5496\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4910\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4630\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4437\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5268\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4760\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5732\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4497\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5219\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5315\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4912\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4451\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5272\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4775\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5358\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5060\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5512\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5237\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5180\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4969\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5205\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5620\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4768\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4763\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4837\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5351\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5164\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4586\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4926\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5507\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5020\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4545\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5546\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4444\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5114\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5566\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5598\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5041\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4800\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5136\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5563\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5144\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5039\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5259\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4472\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4120\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5035\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5394\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5271\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5080\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5377\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4716\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4875\n"
     ]
    }
   ],
   "source": [
    "st = 0\n",
    "en = 1000\n",
    "while(en<=697000):\n",
    "  \n",
    "    batch(st,en)\n",
    "    st = st + 1000\n",
    "    en = en + 1000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 28, 28) for input Tensor(\"flatten_input:0\", shape=(None, 1, 28, 28), dtype=float32), but it was called on an input with incompatible shape (None, 28).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1462 predict_function  *\n        return step_function(self, iterator)\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1452 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1445 run_step  **\n        outputs = model.predict_step(data)\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1418 predict_step\n        return self(x, training=False)\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py:372 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:386 call\n        inputs, training=training, mask=mask)\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:508 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:976 __call__\n        self.name)\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py:216 assert_input_compatibility\n        ' but received input with shape ' + str(shape))\n\n    ValueError: Input 0 of layer dense is incompatible with the layer: expected axis -1 of input shape to have value 784 but received input with shape [None, 28]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d1728df21f2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m~/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1597\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2826\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2828\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2829\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3208\u001b[0m           \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3209\u001b[0m           and call_context_key in self._function_cache.missed):\n\u001b[0;32m-> 3210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_define_function_with_shape_relaxation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3141\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3142\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3143\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1462 predict_function  *\n        return step_function(self, iterator)\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1452 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1445 run_step  **\n        outputs = model.predict_step(data)\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1418 predict_step\n        return self(x, training=False)\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py:372 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:386 call\n        inputs, training=training, mask=mask)\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:508 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:976 __call__\n        self.name)\n    /home/ishaan/miniconda3/envs/ocr/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py:216 assert_input_compatibility\n        ' but received input with shape ' + str(shape))\n\n    ValueError: Input 0 of layer dense is incompatible with the layer: expected axis -1 of input shape to have value 784 but received input with shape [None, 28]\n"
     ]
    }
   ],
   "source": [
    "acc = model.predict(tf.keras.utils.normalize(images[96], axis = 1))\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "save() missing 1 required positional argument: 'filepath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-fd5dede08c85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: save() missing 1 required positional argument: 'filepath'"
     ]
    }
   ],
   "source": [
    "model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: //home//ishaan//Desktop/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('//home//ishaan//Desktop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
